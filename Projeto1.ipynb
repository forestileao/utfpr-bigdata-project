{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a3e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d0f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"5g\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Projeto 1 BigData\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cadb4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.csv(\"./dados_caged_2022.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74196a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------------+------------+----------------+------------------+--------+---------+-----------------+-----+-----------------+--------+----+---------------+--------------------+-----------------+----------------+-------------------------------+--------------------------+--------------+-------------------------------+------------------+-----------------+--------------------+\n",
      "| ano|mes| uf|id_municipio|cnae_2_secao|cnae_2_subclasse|saldo_movimentacao|cbo_2002|categoria|grau_de_instrucao|idade|horas_contratuais|raca_cor|sexo|tipo_empregador|tipo_estabelecimento|tipo_movimentacao|tipo_deficiencia|indicador_trabalho_intermitente|indicador_trabalho_parcial|salario_mensal|tamanho_estabelecimento_janeiro|indicador_aprendiz|origem_informacao|indicador_fora_prazo|\n",
      "+----+---+---+------------+------------+----------------+------------------+--------+---------+-----------------+-----+-----------------+--------+----+---------------+--------------------+-----------------+----------------+-------------------------------+--------------------------+--------------+-------------------------------+------------------+-----------------+--------------------+\n",
      "|2023|  4| SP|     3550308|           H|         4930202|                -1|  841456|      101|                6|   37|             44.0|       1|   1|              0|                   1|               31|               0|                              0|                         0|       2060.89|                              6|                 0|                1|                   0|\n",
      "|2023|  4| SP|     3525904|           H|         4930202|                -1|  414215|      101|                7|   28|             44.0|       2|   1|              0|                   1|               31|               0|                              0|                         0|       2428.16|                             10|                 0|                1|                   0|\n",
      "|2023|  4| SP|     3534401|           H|         4930202|                -1|  414140|      101|                7|   24|             44.0|       2|   1|              0|                   1|               31|               0|                              0|                         0|        1898.0|                             10|                 0|                1|                   0|\n",
      "|2023|  4| SP|     3504008|           H|         4930202|                -1|  414140|      101|                7|   27|             44.0|       1|   1|              0|                   1|               31|               0|                              0|                         0|       2361.87|                              5|                 0|                1|                   0|\n",
      "|2023|  4| SP|     3548708|           H|         4930202|                -1|  783215|      101|                7|   28|             44.0|       1|   1|              0|                   1|               31|               0|                              0|                         0|        1490.7|                              7|                 0|                1|                   0|\n",
      "+----+---+---+------------+------------+----------------+------------------+--------+---------+-----------------+-----+-----------------+--------+----+---------------+--------------------+-----------------+----------------+-------------------------------+--------------------------+--------------+-------------------------------+------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910ec97",
   "metadata": {},
   "source": [
    "# Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa407b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os mapas de parsing\n",
    "sexo_parse_map = {'1': 'HOMEM', '3': 'MULHER', '9': 'NAO_IDENTIFICADO'}\n",
    "tipo_empregador_parse_map = {'0': 'CNPJ', '2': 'CPF', '9': 'NAO_IDENTIFICADO'}\n",
    "saldo_mov_parse_map = {'1': 'ADMITIDO', '-1': 'DEMITIDO'}\n",
    "\n",
    "# Função para aplicar parsing nos DataFrames\n",
    "def parse_dataframe(df):\n",
    "    df = df.drop('id_municipio', 'cbo_2002', 'categoria', 'raca_cor', 'tipo_movimentacao', \n",
    "                 'tipo_deficiencia', 'indicador_trabalho_intermitente', 'indicador_trabalho_parcial', \n",
    "                 'tamanho_estabelecimento_janeiro', 'indicador_aprendiz', 'origem_informacao', \n",
    "                 'indicador_fora_prazo', 'tipo_estabelecimento')\n",
    "\n",
    "    df = df.withColumn('sexo', when(col('sexo') == '1', 'HOMEM')\n",
    "                                .when(col('sexo') == '3', 'MULHER')\n",
    "                                .when(col('sexo') == '9', 'NAO_IDENTIFICADO'))\n",
    "    \n",
    "    df = df.withColumn('tipo_empregador', when(col('tipo_empregador') == '0', 'CNPJ')\n",
    "                                         .when(col('tipo_empregador') == '2', 'CPF')\n",
    "                                         .when(col('tipo_empregador') == '9', 'NAO_IDENTIFICADO'))\n",
    "\n",
    "    df = df.withColumn('saldo_movimentacao', when(col('saldo_movimentacao') == '1', 'ADMITIDO')\n",
    "                                             .when(col('saldo_movimentacao') == '-1', 'DEMITIDO'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Definindo o arquivo de entrada e saída\n",
    "output_filename = './dados_caged_2022_parsed.csv'\n",
    "\n",
    "# Aplicando a função de parsing\n",
    "parsed_df = parse_dataframe(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5db5c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------------+----------------+------------------+-----------------+-----+-----------------+-----+---------------+--------------+\n",
      "| ano|mes| uf|cnae_2_secao|cnae_2_subclasse|saldo_movimentacao|grau_de_instrucao|idade|horas_contratuais| sexo|tipo_empregador|salario_mensal|\n",
      "+----+---+---+------------+----------------+------------------+-----------------+-----+-----------------+-----+---------------+--------------+\n",
      "|2023|  4| SP|           H|         4930202|          DEMITIDO|                6|   37|             44.0|HOMEM|           CNPJ|       2060.89|\n",
      "|2023|  4| SP|           H|         4930202|          DEMITIDO|                7|   28|             44.0|HOMEM|           CNPJ|       2428.16|\n",
      "|2023|  4| SP|           H|         4930202|          DEMITIDO|                7|   24|             44.0|HOMEM|           CNPJ|        1898.0|\n",
      "|2023|  4| SP|           H|         4930202|          DEMITIDO|                7|   27|             44.0|HOMEM|           CNPJ|       2361.87|\n",
      "|2023|  4| SP|           H|         4930202|          DEMITIDO|                7|   28|             44.0|HOMEM|           CNPJ|        1490.7|\n",
      "+----+---+---+------------+----------------+------------------+-----------------+-----+-----------------+-----+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2529656",
   "metadata": {},
   "source": [
    "# Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "751f6136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Função para calcular Z-score e remover outliers\n",
    "def remove_outliers(df, column):\n",
    "    # Calculando a média e desvio padrão\n",
    "    stats = df.select(mean(col(column)).alias('mean'), stddev(col(column)).alias('stddev')).collect()[0]\n",
    "    mean_value = stats['mean']\n",
    "    stddev_value = stats['stddev']\n",
    "\n",
    "    # Adicionando a coluna z_score\n",
    "    df = df.withColumn('z_' + column, (col(column) - mean_value) / stddev_value)\n",
    "\n",
    "    # Filtrando os outliers\n",
    "    df = df.filter((col('z_' + column) <= 3) | col(column).isNull())\n",
    "\n",
    "    return df\n",
    "\n",
    "parsed_df = remove_outliers(parsed_df, 'salario_mensal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c321dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed_df.drop('z_salario_mensal')\n",
    "\n",
    "# Salvando o DataFrame transformado em um novo CSV\n",
    "parsed_df.write.csv(output_filename, header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d24d4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+----+\n",
      "|Sigla| Código|   Município|2022.01|2022.02|2022.03|2022.04|2022.05|2022.06|2022.07|2022.08|2022.09|2022.10|2022.11|2022.12|_c15|\n",
      "+-----+-------+------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+----+\n",
      "|   AC|1200013|  Acrelândia|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|NULL|\n",
      "|   AC|1200054|Assis Brasil|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|NULL|\n",
      "|   AC|1200104|   Brasiléia|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|NULL|\n",
      "|   AC|1200138|      Bujari|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|NULL|\n",
      "|   AC|1200179|    Capixaba|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|NULL|\n",
      "+-----+-------+------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 23:57:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Sigla, Código, Município, 2022.01, 2022.02, 2022.03, 2022.04, 2022.05, 2022.06, 2022.07, 2022.08, 2022.09, 2022.10, 2022.11, 2022.12, \n",
      " Schema: Sigla, Código, Município, 2022.01, 2022.02, 2022.03, 2022.04, 2022.05, 2022.06, 2022.07, 2022.08, 2022.09, 2022.10, 2022.11, 2022.12, _c15\n",
      "Expected: _c15 but found: \n",
      "CSV file: file:///home/hellhat/Documents/facul/BigData/cesta_basica_2022.csv\n"
     ]
    }
   ],
   "source": [
    "cb_df = spark.read.csv('./cesta_basica_2022.csv', header=True, sep=',') \n",
    "\n",
    "cb_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c811812",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_df = cb_df.drop('_c15')\n",
    "cb_df = cb_df.filter(cb_df[\"`2022.01`\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d669340",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_columns = [f\"`2022.{str(i).zfill(2)}`\" for i in range(1, 13)]\n",
    "monthly_columns_no_parsing = [f\"2022.{str(i).zfill(2)}\" for i in range(1, 13)]\n",
    "\n",
    "\n",
    "# Calculate the mean of the monthly columns\n",
    "cb_df = cb_df.withColumn(\"media_cesta_basica\", sum(col(c) for c in monthly_columns) / len(monthly_columns))\n",
    "\n",
    "# Drop the original monthly columns\n",
    "cb_df = cb_df.drop(*monthly_columns_no_parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34d7de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Sigla|media_cesta_basica|\n",
      "+-----+------------------+\n",
      "|   BA| 566.3316666666667|\n",
      "|   CE|             631.8|\n",
      "|   DF| 699.4575000000001|\n",
      "|   ES| 701.6608333333334|\n",
      "|   GO|          668.6175|\n",
      "|   MG|          661.0075|\n",
      "|   MS| 713.1366666666667|\n",
      "|   PA| 613.7741666666667|\n",
      "|   PB|            563.41|\n",
      "|   PE| 576.2008333333334|\n",
      "|   PR| 691.9408333333332|\n",
      "|   RJ|             730.0|\n",
      "|   RN| 579.3108333333332|\n",
      "|   RS|          747.3175|\n",
      "|   SC| 751.2283333333334|\n",
      "|   SE| 529.0558333333335|\n",
      "|   SP| 762.2308333333331|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 00:02:04 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:15 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:18 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:19 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:19 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:19 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:19 WARN BasicWriteTaskStatsTracker: Expected 1 files, but only saw 0. This could be due to the output format not writing empty files, or files being not immediately visible in the filesystem.\n",
      "24/05/26 00:02:19 ERROR FileFormatWriter: Aborting job 854619ca-df67-4aea-a0ab-f7a722744b21.\n",
      "java.io.FileNotFoundException: File file:/home/hellhat/Documents/facul/BigData/parsed_cesta_basica.csv/_temporary/0 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "cb_df = cb_df.drop('Código', 'Município')\n",
    "cb_df.show(30)\n",
    "cb_df.write.csv('parsed_cesta_basica.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe61cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
